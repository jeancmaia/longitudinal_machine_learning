{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chapter itemize a time series into three components: `trend`, `seasonal` and `residual`. This chapter considers the later, a.k.a `cycle`, in all of the pivotal traits thoroughly developed. A beginner yet fundamental model leverages `White Noise` as building blocks. Let $y_t$ calculated as:\n",
    "\n",
    "$$y_t = \\epsilon_t $$ \n",
    "$$ \\epsilon_t \\sim N(0, \\sigma ^2 ) $$\n",
    "\n",
    "The \"error\" $\\epsilon$ is uncorrelated over time. That process with zero mean, constant variance, and no correlation, is known as `zero-mean gaussian white noise`. Therefore: \n",
    "\n",
    "$$ y_t \\sim WN(0, \\sigma ^2 ) $$\n",
    "\n",
    "In addition to it, autocovariances structures are smoothly calculated. Let $\\tau$ as a displacement, the autocovariance function $\\gamma$ is. \n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma(\\tau)=\n",
    "\\begin{cases}\n",
    "  \\sigma ^2, & \\tau = 0\n",
    "\\\\\n",
    "  0, & \\tau \\geq 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The autocorrelation function for a white noise:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho(\\tau)=\n",
    "\\begin{cases}\n",
    "  \\sigma ^2, & \\tau = 0\n",
    "\\\\\n",
    "  0, & \\tau \\geq 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the partial autocorrelation function is:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\tau)=\n",
    "\\begin{cases}\n",
    "  \\sigma ^2, & \\tau = 0\n",
    "\\\\\n",
    "  0, & \\tau \\geq 1\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've itemized white noise in terms of its mean, variance, autocorrelation, and partial autocorrelation function. Notwithstanding the staggering magnitude of these traits, they don't craft an azimuth to takeaways from both the first and second moments regarding the process; the past `conditional` mean and variance do. We shall distinguish `unconditional` and `conditional` mean and variance.    \n",
    "\n",
    "\n",
    "Let the conditioned mean and variance upon the information $\\Omega_{t-1}$, which contains a past history of the observed series. It might leverage observed series, $\\Omega_{t-1}$ = $y_{t-1}$, $y_{t-2}$,...; or the residuals(shocks), $\\Omega_{t-1}$ = $\\epsilon_{t-1}$, $\\epsilon_{t-2}$,... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to Estimation and Inference for the Mean, Autocorrelation and Partial Autocorrelation Functions.   \n",
    "\n",
    "1) The estimator for the population mean, given a sample of size T.   \n",
    "\n",
    "\n",
    "$$ \\bar{y} = \\dfrac{1}{Y}\\sum_{t=1}^{T} y_t $$   \n",
    "\n",
    "\n",
    "2) Sample Autocorrelations.   \n",
    "\n",
    "$$ \\hat{\\rho}(\\tau) = \\dfrac{\\sum_{t=\\tau+1}^{T}[(y_t-\\hat{y})(y_{t-\\tau}-\\hat{y}) ]}{\\sum_{t=1}^{T}(y_t-\\hat{y})^2}. $$\n",
    "\n",
    "\n",
    "It is usual to assess whether a series is reasonably a white noise. The distribution of the sample autocorrelations in large samples is:    \n",
    "\n",
    "\n",
    "$$ \\hat{\\rho}(\\tau) \\sim N(0, \\dfrac{1}{T} ). $$\n",
    "\n",
    "\n",
    "Under normality, the 95% confidence interval is build by two standard errors around the mean, making up the boundaries: $0 \\pm 2/\\sqrt{T}$.    \n",
    "\n",
    "3) Sample Partial Autocorrelations.    \n",
    "\n",
    "\n",
    "They are obtained from population linear regressions. If the fitted regression is:\n",
    "\n",
    "\n",
    "$$ \\hat{y_t} = \\hat{c} + \\hat{\\beta_1}y_{t-1} + ... + \\hat{\\beta_\\tau}y_{t-\\tau}. $$\n",
    "\n",
    "\n",
    "the `sample partial autocorrelation` at displacement $\\tau$ is:\n",
    "\n",
    "$$ \\hat{p}(\\tau) \\equiv \\hat{\\beta_\\tau}. $$\n",
    "\n",
    "A straightforward solution for this is by least squares analytical solution.\n",
    "\n",
    "$$ \\beta=(X^TX)^{-1}X^Ty.$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_white_noise(size: int):\n",
    "    return np.random.normal(loc=0, scale=0.5, size=size)\n",
    "\n",
    "def mean_estimator(time_series: np.array):\n",
    "    return np.mean(time_series)\n",
    "\n",
    "def sample_autocorrelation(time_series: np.array, tau: int):\n",
    "    y_hat = mean_estimator(time_series)\n",
    "    residuals = time_series - y_hat\n",
    "    \n",
    "    numerator = np.sum(residuals[tau+1:] * residuals[1:len(residuals)-tau])\n",
    "    denominator = np.sum(residuals[1:]**2)      \n",
    "      \n",
    "    return numerator/denominator\n",
    "\n",
    "from numpy.linalg import inv, solve\n",
    "\n",
    "def sample_partial_autocorrelation(X_lags : np.array, y : np.array):\n",
    "    return np.dot(inv(np.dot(X_lags.T, X_lags)), np.dot(X_lags.T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.026428980620308646\n",
      "autocorrelation: 1.0\n"
     ]
    }
   ],
   "source": [
    "sample_syntetic = generate_white_noise(40)\n",
    "\n",
    "print('mean:', mean_estimator(sample_syntetic))\n",
    "print('autocorrelation:', sample_autocorrelation(sample_syntetic, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.lag import Lag\n",
    "\n",
    "## lags for a partial autocorrelation plot for a p=5 analysis.\n",
    "t = Lag([1, 2, 3, 4, 5])\n",
    "Xt = t.fit_transform(sample_syntetic)\n",
    "sample_syntetic_lags = Xt[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19242129, -0.21358846,  0.01239841, -0.10850784,  0.11303483])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_partial_autocorrelation(sample_syntetic_lags[5:], sample_syntetic[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Lag Operator` lies in the gritty of time series models. It operates on a series by lagging it. It is usual to operate not with lag operator, but with its polynomial. A lag operator of degree m is just a linear function of powers of $L$, up through the $m-th$ power.\n",
    "\n",
    "$$ B(L) = b_0 + b_1L + b_2L^2 + ... + b_mL^m.$$\n",
    "\n",
    "Furthermore, infinite-order polynomials are super useful as well. Models involving infinite distributed lags are central to time series modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Processes\n",
    "\n",
    "\n",
    "It is a natural model for cycles. In AR Model, the current value is linearly related to its past values and a stochastic residual/shock. The first-order autoregressive process, AR(1), is:\n",
    "\n",
    "$$y_t = \\phi y_{t-1} + \\epsilon_t$$\n",
    "$$\\epsilon_t \\sim WN(0, \\sigma^2) $$\n",
    "\n",
    "Even a simple AR(1) can be plastic and resilient. Let's take a peek at the shapes of two simulations with parameters $\\sigma$ as `0.95` and `0.4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.5 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "ar1 = np.array([1, 0.9])\n",
    "ma1 = np.array([1])\n",
    "AR_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = AR_object1.generate_sample(nsample=150)\n",
    "plt.plot(simulated_data_1);\n",
    "\n",
    "# Plot 2: AR parameter = -0.9\n",
    "plt.subplot(2, 1, 2)\n",
    "ar2 = np.array([1, 0.4])\n",
    "ma2 = np.array([1])\n",
    "AR_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = AR_object2.generate_sample(nsample=150)\n",
    "plt.plot(simulated_data_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a natural expasion for the Autoregressive model. Assuming the AR(1) process, written as:\n",
    "\n",
    "$$ y_t = \\phi y_{t-1} + \\epsilon_t$$\n",
    "\n",
    "if we replace the lagged y's with backward, we end up with the following model.\n",
    "\n",
    "$$ y_t = \\phi y_{t-2} + \\phi \\epsilon_{t-1}  + \\epsilon_t $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$ y_t = \\epsilon_t + \\phi \\epsilon_{t-1} + \\phi^2 \\epsilon_{t-2} + ...$$\n",
    "\n",
    "This is the `moving-average model` representation.  \n",
    "\n",
    "That model is intuitive. As long as $\\epsilon$'s are the components that move $y$, we would be able to express $y$ in terms of the history of $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The AR(p) Process \n",
    "\n",
    "The general p-th order autoregressive process is:\n",
    "\n",
    "$$y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t $$\n",
    "$$ \\epsilon_t \\sim WN(0, \\sigma^2) $$\n",
    "\n",
    "\n",
    "Broadly speaking, the autocorrelation function AR(p) decays gradually throughout displacement. In addition to it, an AR(p) process is covariance stationary if and only if all roots of lag operator polynomial are outside the unit circle.\n",
    "\n",
    "$$ \\Phi (L)y_t = (1 - \\phi_1 L - \\phi_2 L^2 - ... - \\phi_p L^p)y_t. $$ \n",
    "\n",
    "\n",
    "----- \n",
    "##### Forecasting Cycles with Autoregressions\n",
    "\n",
    "\n",
    "The first key component is the `feature set`, which holds the available historical data. The T-time information set $\\Omega_T$ is represented as:\n",
    "\n",
    "$$ \\Omega_T = {y_T, y_{T-1}, y_{T-2}, ...} $$\n",
    "\n",
    "We aim to project the `optimal forecast` for a future time `T + h`, given the `information set`. The optimal is the that one minimizes the expected loss, or the conditional mean. \n",
    "\n",
    "$$ E(y_{T+H} | \\Omega_T) $$\n",
    "\n",
    "The conditional mean doesn't need to be a linear function; however, for AR sake, it is. As long as forecasts are linear projections in the elements on the information set, denoted as\n",
    "\n",
    "$$ P(y_{T+h} | \\Omega_T) $$ \n",
    "\n",
    "In the Gaussian case, the conditional expectation is exactly linear, so that.\n",
    "\n",
    "$$ E(y_{T + h} | \\Omega_T) = P(y_{T + h} | \\Omega_T ) $$\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "#### Point Forecasts for Autoregressions\n",
    "\n",
    "Taking an AR(1) process, $y_t = \\phi y_{t-1} + \\epsilon_t$; $\\epsilon \\sim WN(0, \\sigma^2)$.\n",
    "\n",
    "To construct a 1-step-ahead forecast, the preocess for T + 1,\n",
    "\n",
    "$$y_{T+1} = \\phi y_{T} + \\epsilon_{T+1}. $$\n",
    "\n",
    "Transforming the right-hand side on the time-T information set.\n",
    "\n",
    "$$y_{T+1,T} = \\phi y_{T}. $$\n",
    "\n",
    "Then, for the prediction of h displacement, \n",
    "\n",
    "$$y_{T+h,T} = \\phi^h y_{T}. $$\n",
    "\n",
    "As long as Normality is an assumption, we may draw a h-step-ahead forecast by means of a Gaussian realization with parameters:\n",
    "\n",
    "$$ y_{T+h} \\sim N(y_{T+h,T}, \\sigma_h^2), $$\n",
    "\n",
    "where $\\sigma_h² = var(y_{T+h}|\\Omega_{T})$ and $\\Omega_T$ = {$y_T, y_{T-1}, ...$}. Using Wold's chain rule, the variance for a h-step-ahead forecast error variance, $\\sigma_h^2$, for an AR(1) process.\n",
    "\n",
    "\n",
    "$$ \\sigma_1^2 = \\sigma^2 $$\n",
    "$$ \\sigma_2^2 = \\sigma^2(1 + \\phi^2)$$\n",
    "$$ \\sigma_h^2 = \\sigma^2 \\sum_{i=0}^{h-1} \\phi^{2i}. $$\n",
    "\n",
    "\n",
    "By a known convergence, we it is possible to assess:\n",
    "\n",
    "$$\\lim_{x\\to\\infty} \\sigma_h ^2 = \\frac{\\sigma^2}{1-\\sigma^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "To knock off this section, we instill on a crucial concept regarding covariance stationary time-series, the `Wold Representation`. To recap and motivate the subject, a time-series can be decomposed into three main components: Trend, Seasonal and Residual, in which the later is generally `a zero-mean covariance-stationary process`. We present the `Wold's representation theorem` as the appropriate model to fit that series.\n",
    "\n",
    "Theorem:\n",
    "\n",
    "Let {yt} a zero-mean covariance-stationary process, we can write it as:\n",
    "\n",
    "$$ y_t = B(L) \\epsilon_t = \\sum_{i=0}^{\\infty} b_i \\epsilon_{t-1} $$\n",
    "$$ \\epsilon_t \\sim WN(0, \\sigma^2) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$b_0 = 1$$\n",
    "$$\\sum_{i=0}^{\\infty} b_i^2  < \\infty$$\n",
    "\n",
    "\n",
    "The `Wold representation` is an infinite distributed lag of white noise. The $\\epsilon_t's$ are often called `innovations` for a good forecast, and they represent the part that is linearly unpredictable on the basis of the past of y.  \n",
    "\n",
    "In the statistics literature, three models can approximate accurately and seamlessly to the `Wold's representation`: moving average models, autoregressive models and autoregressive moving average models. As we've dissected in this chapter, Autoregressive and Moving average models leverage past observations and white noises to explain the current observation, respectivelly. A ARMA model harnesses both at same time.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ARMA process combines two linear process: The Auto Regressive and the Moving Average, parameterized by two parameters: `p` e `q`. The mathematical notation goes as: \n",
    "\n",
    "$y_t - \\phi_1y_{t-1} - ... - \\phi_py_{t-p} = \\epsilon_t + \\theta_1\\epsilon_{t-1} + ... + \\theta_q\\epsilon_{t-q} $\n",
    "\n",
    "Therefore an ARMA leverages two polynomials $\\phi$ and $\\theta$ and their operation with past $x_{t-p}$ and $\\epsilon_{t-q}$ values. On a vernacular tongue, the AR part relates the current value with the last results of the series, whereas the MA relates with respective random components of the oldest.\n",
    "\n",
    "Two key concepts that raise here is `causal` and `independent`, which are deeply dig on the definitive guides.    \n",
    "\n",
    "\n",
    "The properties of `point forecast` are similar to the AR and can easily be transmutated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------- \n",
    "\n",
    "### ARMA process\n",
    "\n",
    "A ARMA(p,q) process is the foremost model for cycles. Conceptually, it generalizes and represents an AR process of infinite-order. To establish a polynomial representation, \n",
    "\n",
    "$$ \\Phi (L)^p y_t =  \\theta(L)^q \\epsilon_t,$$ \n",
    "\n",
    "where the parameter `p` the sets maximum displacement for `AR`, whereas `q` for the `MA` model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
