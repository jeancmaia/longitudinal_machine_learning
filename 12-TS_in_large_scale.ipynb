{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-stationary Transformers:  https://t.co/HoFxmHoaT2   \n",
    "Probabilistic Time Series Forecasting with Transformers: https://huggingface.co/blog/time-series-transformers   \n",
    "Stock Embeddings for Portfolios: https://t.co/YuK5u8gXHz   \n",
    "Multi-Order Dynamics and Internal Dynamics in Stock Movement: https://t.co/wyuva2EnG3  \n",
    "Tree-Based Local Volatility Surface: https://impa.br/wp-content/uploads/2018/10/projetos_fim_curso_yuri_resende_fonseca.pdf   \n",
    "Random Forests and Boosting for ARCH-like volatility forecasts: https://www.sarem-seitz.com/random-forests-and-boosting-for-arch-like-volatility-forecasts/     \n",
    "Deep Learning For Time Serie Forecasting: Tutorial and Literatue Survey.  https://arxiv.org/pdf/2004.10240.pdf   \n",
    "\n",
    "The Effectiveness of Discretization in Forecasting: An Empirical Study on Neural Time Series Models. https://arxiv.org/abs/2005.10111    \n",
    "https://arxiv.org/abs/1809.04356       \n",
    "https://arxiv.org/pdf/2102.12658.pdf     \n",
    "https://arxiv.org/abs/2012.12485    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling time series at a large scale is challenging and resource-intensive. Most of the classical literature is devoted to univariate, either heteroscedasticity or homoscedasticity, and multivariate, where the joint covariance among distinct realizations is a matter of study; however, with unclear limitations about the volume. Deep Learning raises a trailblazing proposal for scaling it up. Thus, in this chapter, we introduce global big models for time series with DL.\n",
    "\n",
    "As with the others, this notebook is a personal summary of excellent papers, the aforementioned ones. If you need clarification or deeper details, please reach out to the bare references.     \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "##### The Deep Learning Era\n",
    "\n",
    "Deep Learning is a game-changer in many areas, such as Computer Vision and NLP. By 2006, (Hinton et al) presented to the world a feasible footstep for training Neural Networks with a large number of layers - if, and only if, initial weights are set adequately. Nowadays, DL is a ubiquitous solution, even in Forecast solutions - indeed, owing to its traits, they are a fruitful field for DL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before dwelling on the literature and candidate models, we shed light on the cornerstone of time series: stationarity. Distribution-varying time series realization makes deep learning models intractable. Therefore, applying pre-processing data for deep model inputs is a standard approach [1]. To cite a few: Z-score normalization, DAIN, and RevIN. Let's apply those techniques in practice.\n",
    "\n",
    "\n",
    "[1] Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting. https://arxiv.org/pdf/2205.14415.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mglu = pd.read_csv('MGLU3_historical_data.csv')\n",
    "\n",
    "mglu['Date'] = pd.to_datetime(mglu['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A amazing DAIN implementation with Pytorch was provided by:\n",
    "\n",
    "https://doi.org/10.5281/zenodo.4066525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08c0719674ff094177ff71caf104755cc919f8ecb4d7e92f9d6bcb4bedf87c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
